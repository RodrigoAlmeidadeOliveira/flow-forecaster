#!/usr/bin/env python3
"""
Test Script for Async Architecture
Tests Celery tasks, progress updates, and cancellation
"""
import os
import sys
import time
import json
from datetime import datetime

# Set environment variables
os.environ['DATABASE_URL'] = 'sqlite:///forecaster.db'
os.environ['CELERY_BROKER_URL'] = 'redis://localhost:6379/0'
os.environ['CELERY_RESULT_BACKEND'] = 'redis://localhost:6379/0'
os.environ['FLOW_FORECASTER_SECRET_KEY'] = 'test-secret-key'

# Import after setting env vars
from celery_app import celery_app
from tasks.simulation_tasks import (
    run_monte_carlo_async,
    health_check
)

def print_section(title):
    """Print formatted section header"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)

def print_status(message, symbol="‚úì"):
    """Print status message"""
    print(f"  {symbol} {message}")

def test_health_check():
    """Test 1: Health check task"""
    print_section("TEST 1: Health Check")

    try:
        # Submit health check task
        print_status("Submitting health check task...", "‚è≥")
        task = health_check.delay()

        print_status(f"Task ID: {task.id}", "üìã")

        # Wait for result
        result = task.get(timeout=10)

        print_status("Health check completed!", "‚úÖ")
        print(f"     Worker: {result.get('worker', 'unknown')}")
        print(f"     Status: {result.get('status', 'unknown')}")
        print(f"     Timestamp: {result.get('timestamp', 'unknown')}")

        return True

    except Exception as e:
        print_status(f"Health check failed: {e}", "‚ùå")
        return False

def test_monte_carlo_simulation():
    """Test 2: Monte Carlo simulation"""
    print_section("TEST 2: Monte Carlo Simulation (Async)")

    try:
        # Prepare test data
        simulation_data = {
            'projectName': 'Test Project - Async',
            'numberOfSimulations': 1000,  # Reduced for faster testing
            'confidenceLevel': 85,
            'tpSamples': [5, 6, 7, 8, 9, 10],
            'numberOfTasks': 20,
            'totalContributors': 3,
            'minContributors': 3,
            'maxContributors': 3,
            'ltSamples': [],
            'splitRateSamples': [],
            'risks': [],
            'dependencies': [],
            'teamFocus': 1.0,
            'startDate': '01/01/2025',
            'sCurveSize': 3  # Required parameter
        }

        print_status("Submitting Monte Carlo simulation...", "‚è≥")
        print(f"     Simulations: {simulation_data['numberOfSimulations']}")
        print(f"     Tasks: {simulation_data['numberOfTasks']}")
        print(f"     Team: {simulation_data['totalContributors']} people")

        # Submit task
        task = run_monte_carlo_async.delay(
            simulation_data=simulation_data,
            user_id=None,
            save_forecast=False
        )

        print_status(f"Task ID: {task.id}", "üìã")

        # Poll for status
        print_status("Polling for progress...", "üîÑ")

        start_time = time.time()
        last_progress = -1

        while not task.ready():
            # Get task state
            state = task.state
            info = task.info if isinstance(task.info, dict) else {}

            if state == 'PROGRESS':
                progress = info.get('progress', 0)
                stage = info.get('stage', 'Processing')
                status = info.get('status', '')

                if progress != last_progress:
                    elapsed = time.time() - start_time
                    print(f"     [{progress:3d}%] {stage} - {status} ({elapsed:.1f}s)")
                    last_progress = progress

            time.sleep(0.5)

        # Get result
        task_result = task.get(timeout=5)
        elapsed = time.time() - start_time

        print_status(f"Simulation completed in {elapsed:.2f}s!", "‚úÖ")

        # Extract actual result from task metadata
        result = task_result.get('result', task_result) if isinstance(task_result, dict) else task_result

        # Debug
        print(f"     Task result type: {type(task_result)}")
        if isinstance(task_result, dict):
            print(f"     Task result keys: {list(task_result.keys())}")
        print(f"     Result type: {type(result)}")
        if isinstance(result, dict):
            print(f"     Result keys: {list(result.keys())[:10]}...")  # First 10 keys

        # Validate result (accepts multiple output formats)
        has_simulations = 'simulations' in result or 'distribution' in result
        has_stats = 'percentile_stats' in result or 'percentiles' in result

        if isinstance(result, dict) and has_simulations:
            print_status("Result validation:", "‚úì")

            # Get simulations data
            simulations = result.get('simulations', result.get('distribution', []))
            print(f"     Simulations completed: {len(simulations)}")

            # Get stats
            stats = result.get('percentile_stats', result.get('percentiles', {}))
            if stats:
                p50 = stats.get('p50', stats.get('50%', 0))
                p85 = stats.get('p85', stats.get('85%', 0))
                print(f"     P50: {p50:.2f} weeks")
                print(f"     P85: {p85:.2f} weeks")

            # Check if task_id is present (added by our async wrapper)
            if 'task_id' in result:
                print(f"     Task ID in result: {result['task_id'][:8]}...")

            return True
        else:
            print_status("Result validation failed - missing simulation data", "‚ùå")
            print(f"     Has simulations: {has_simulations}")
            print(f"     Has stats: {has_stats}")
            return False

    except Exception as e:
        print_status(f"Simulation failed: {e}", "‚ùå")
        import traceback
        traceback.print_exc()
        return False

def test_task_cancellation():
    """Test 3: Task cancellation"""
    print_section("TEST 3: Task Cancellation")

    try:
        # Submit long-running task
        simulation_data = {
            'projectName': 'Cancellation Test',
            'numberOfSimulations': 50000,  # Long-running
            'confidenceLevel': 85,
            'tpSamples': [5, 6, 7],
            'numberOfTasks': 100,
            'totalContributors': 5,
            'minContributors': 5,
            'maxContributors': 5,
            'ltSamples': [],
            'splitRateSamples': [],
            'risks': [],
            'dependencies': [],
            'teamFocus': 1.0,
            'startDate': '01/01/2025',
            'sCurveSize': 5  # Required parameter
        }

        print_status("Submitting long-running task...", "‚è≥")
        task = run_monte_carlo_async.delay(
            simulation_data=simulation_data,
            user_id=None,
            save_forecast=False
        )

        print_status(f"Task ID: {task.id}", "üìã")

        # Wait a bit
        time.sleep(2)
        print_status("Waiting for task to start...", "‚è±Ô∏è")
        time.sleep(2)

        # Cancel task
        print_status("Cancelling task...", "üõë")
        task.revoke(terminate=True)

        # Wait and check status
        time.sleep(2)
        state = task.state

        if state in ['REVOKED', 'FAILURE']:
            print_status(f"Task successfully cancelled (state: {state})", "‚úÖ")
            return True
        elif state == 'SUCCESS':
            # Task completed before we could cancel - that's OK for fast simulations
            print_status(f"Task completed before cancellation (state: {state})", "‚úì")
            print_status("Cancellation test skipped - task too fast", "‚ö†Ô∏è")
            return True  # Not a failure, just couldn't test cancellation
        else:
            print_status(f"Task in unexpected state: {state}", "‚ùå")
            return False

    except Exception as e:
        print_status(f"Cancellation test failed: {e}", "‚ùå")
        return False

def test_redis_connection():
    """Test Redis connection"""
    print_section("TEST 0: Infrastructure Check")

    try:
        # Test Redis
        from redis import Redis
        r = Redis(host='localhost', port=6379, db=0)
        r.ping()
        print_status("Redis connection: OK", "‚úÖ")

        # Test Celery broker connection
        from celery_app import celery_app
        i = celery_app.control.inspect()
        stats = i.stats()

        if stats:
            print_status(f"Celery workers: {len(stats)} active", "‚úÖ")
            for worker, data in stats.items():
                print(f"     - {worker}: {data.get('pool', {}).get('max-concurrency', 'N/A')} workers")
        else:
            print_status("Celery workers: None found", "‚ö†Ô∏è")

        return True

    except Exception as e:
        print_status(f"Infrastructure check failed: {e}", "‚ùå")
        return False

def main():
    """Run all tests"""
    print("\n" + "=" * 70)
    print("  ASYNC ARCHITECTURE - INTEGRATION TESTS")
    print("  Flow Forecaster - Background Jobs with Celery")
    print("=" * 70)
    print(f"  Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  Python: {sys.version.split()[0]}")
    print("=" * 70)

    results = {}

    # Test 0: Infrastructure
    results['infrastructure'] = test_redis_connection()

    if not results['infrastructure']:
        print_section("CRITICAL ERROR")
        print_status("Infrastructure not ready. Aborting tests.", "‚ùå")
        return

    # Test 1: Health check
    results['health_check'] = test_health_check()

    # Test 2: Monte Carlo simulation
    results['monte_carlo'] = test_monte_carlo_simulation()

    # Test 3: Cancellation
    results['cancellation'] = test_task_cancellation()

    # Summary
    print_section("TEST SUMMARY")

    total = len(results)
    passed = sum(1 for v in results.values() if v)
    failed = total - passed

    for test_name, passed_test in results.items():
        symbol = "‚úÖ" if passed_test else "‚ùå"
        status = "PASSED" if passed_test else "FAILED"
        print(f"  {symbol} {test_name.replace('_', ' ').title()}: {status}")

    print("\n" + "-" * 70)
    print(f"  Total: {total} tests | Passed: {passed} | Failed: {failed}")
    print("-" * 70)

    if failed == 0:
        print("\n  üéâ ALL TESTS PASSED! Async architecture is working correctly! üéâ\n")
        return 0
    else:
        print(f"\n  ‚ö†Ô∏è  {failed} test(s) failed. Please review the output above.\n")
        return 1

if __name__ == '__main__':
    sys.exit(main())
