#!/usr/bin/env python3
"""
Test ML validation hotfix for minimum sample requirements.
Tests that endpoints properly reject requests with < 15 samples.
"""

def test_sample_validation():
    """
    Test that the validation logic correctly identifies insufficient data.
    """
    test_cases = [
        {
            'name': 'Workshop Grupo 3 (8 samples)',
            'samples': [8, 6, 10, 7, 9, 8, 5, 11],
            'should_pass': False,
            'expected_error': 'insufficient_data'
        },
        {
            'name': 'Workshop Grupo 1 (10 samples)',
            'samples': [6, 8, 5, 9, 7, 6, 10, 7, 8, 6],
            'should_pass': False,
            'expected_error': 'insufficient_data'
        },
        {
            'name': 'Workshop Grupo 2 (12 samples)',
            'samples': [8, 6, 10, 7, 9, 8, 5, 11, 7, 9, 8, 10],
            'should_pass': False,
            'expected_error': 'insufficient_data'
        },
        {
            'name': 'Minimum threshold (15 samples)',
            'samples': [8, 6, 10, 7, 9, 8, 5, 11, 7, 9, 8, 10, 7, 5, 9],
            'should_pass': True,
            'expected_error': None
        },
        {
            'name': 'Good dataset (20 samples)',
            'samples': [8, 6, 10, 7, 9, 8, 5, 11, 7, 9, 8, 10, 7, 5, 9, 8, 11, 6, 10, 7],
            'should_pass': True,
            'expected_error': None
        }
    ]

    print("ðŸ§ª Testing ML Validation Hotfix\n")
    print("=" * 70)

    all_passed = True

    for test_case in test_cases:
        samples = test_case['samples']
        n_samples = len(samples)
        should_pass = test_case['should_pass']

        # Simulate the validation logic from app.py
        if not samples or len(samples) < 15:
            validation_result = {
                'passed': False,
                'error': 'insufficient_data',
                'message': f'Machine Learning requires at least 15 samples for reliable results. You provided {len(samples)} samples.',
                'recommendation': 'Use Monte Carlo simulation instead, which works well with 5+ samples.',
                'min_required': 15,
                'provided': len(samples)
            }
        else:
            validation_result = {
                'passed': True,
                'error': None
            }

        # Check if result matches expectation
        test_passed = validation_result['passed'] == should_pass

        # Print result
        status = "âœ… PASS" if test_passed else "âŒ FAIL"
        print(f"\n{status} | {test_case['name']}")
        print(f"   Samples: {n_samples}")
        print(f"   Expected: {'Accept' if should_pass else 'Reject'}")
        print(f"   Got: {'Accept' if validation_result['passed'] else 'Reject'}")

        if not validation_result['passed']:
            print(f"   Error: {validation_result['error']}")
            print(f"   Message: {validation_result['message']}")

        if not test_passed:
            all_passed = False

    print("\n" + "=" * 70)

    if all_passed:
        print("âœ… ALL TESTS PASSED - Hotfix working correctly!")
        print("\nðŸ“‹ Summary:")
        print("   - Workshop scenarios (8-12 samples) will be REJECTED âœ“")
        print("   - Minimum threshold (15 samples) will be ACCEPTED âœ“")
        print("   - Good datasets (20+ samples) will be ACCEPTED âœ“")
        return 0
    else:
        print("âŒ SOME TESTS FAILED - Review validation logic")
        return 1

if __name__ == '__main__':
    import sys
    exit_code = test_sample_validation()
    sys.exit(exit_code)
