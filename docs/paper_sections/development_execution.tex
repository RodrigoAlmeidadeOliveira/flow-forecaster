\section{Development and Execution}
The Flow Forecaster implementation follows a layered architecture that separates presentation, orchestration, and analytical concerns. The Flask application in \texttt{app.py} exposes REST endpoints (e.g., \texttt{/api/mc-throughput}, \texttt{/api/ml-forecast}, \texttt{/api/combined-forecast}) and renders interactive dashboards via \texttt{templates/index.html}. Request handlers parse user-supplied JSON payloads containing historical throughput samples, backlog sizes, risk descriptions, and forecasting horizons. They then delegate to specialized services: Monte Carlo simulations are served by \texttt{monte\_carlo\_unified.py}, machine learning workflows by \texttt{ml\_forecaster.py}, deadline analyses by \texttt{analyze\_deadline} (within the same module), and portfolio health checks by \texttt{portfolio\_analyzer.py}. Authentication and persistence rely on SQLAlchemy models (\texttt{models.py}) and helper routines that manage scoped sessions per user.

\subsection{Monte Carlo Pipeline}
The Monte Carlo engine adapts and extends the original JavaScript implementation that powered the browser version of the tool. It exposes unified routines to simulate throughput-based project completion, compute probabilistic capacity tables, and generate risk summaries. Each simulation performs $10{,}000$ stochastic draws by default, resampling historical throughput arrays with optional smoothing, modeling ramp-up phases through S-curves, and incorporating discrete risk events. The \texttt{simulate\_throughput\_forecast} function returns percentile statistics (P10 through P95), expected completion weeks, and scenario metadata that the UI renders via Chart.js. Auxiliary utilities compute error rates, derive descriptive statistics, and support deadline-focused functions (\texttt{forecast\_how\_many}, \texttt{forecast\_when}) that map capacity forecasts onto calendar intervals.

Regression tests (\texttt{test\_probability\_fix.py}, \texttt{test\_deadline\_fix.py}) operationalize this pipeline. They exercise the "How many?" and "When?" analyses with synthetic data, verifying that percentile outputs, backlog coverage, and deadline feasibility checks remain consistent. For instance, when simulating a 25-item backlog with the reference throughput sequence, the P85 capacity exceeded the backlog within a four-week horizon, and the deadline module reported sufficient lead time---a behavior cross-validated against manual calculations in the scripts. These tests also guard the interpretation of probability tables, enforcing that a 100\% probability corresponds to the minimum observed outcome while a 1\% probability maps to the upper tail.

\subsection{Machine Learning Pipeline}
The ML subsystem is encapsulated in the \texttt{MLForecaster} class. It begins by transforming historical throughput arrays into supervised-learning datasets via lag features, rolling means, and rolling standard deviations. The training process instantiates a model zoo that includes Random Forest, HistGradient Boosting (median plus quantile variants at 10th and 90th percentiles), Ridge, Lasso, $k$-Nearest Neighbors, Support Vector Regression, and XGBoost when the optional dependency is installed. For each model, the pipeline executes a two-stage protocol: (i) a grid or randomized search on an 80/20 temporal split to select hyperparameters aligned with time-series constraints, and (ii) a $k$-fold cross-validation (default $k=5$) on the training portion to capture variance in MAE, RMSE, and $R^2$ metrics. Models are then retrained on the full dataset and stored alongside their validation statistics and optimal parameters.

Operational diagnostics continue with walk-forward validation, implemented inside \texttt{walk\_forward\_validation}. The routine iteratively retrains models at multiple forecast origins, comparing predicted throughput trajectories against held-out actuals and reporting origin-level MAE, RMSE, and MAPE. Dependency-aware adjustments, when configured, inflate predicted timelines according to expected delay weeks derived from the optional \texttt{dependency\_analyzer}. Results are formatted for both tabular presentation and chart rendering in the frontend, enabling analysts to inspect per-model drift or degradation.

\subsection{Execution Environment and Tooling}
The repository bundles scripts and notebooks that automate deployment and experimentation. Docker and Fly.io manifests configure the application to bind to port 8080, exposing Gunicorn-managed workers tuned for computationally expensive simulations. The testing toolchain uses \texttt{pytest} compatible scripts, while continuous experimentation is facilitated by manual runners (\texttt{test\_forecast\_vs\_actual.py}) that aggregate API endpoints, walk-forward metrics, and visualization artifacts. Throughout our analysis, we relied exclusively on the bundled synthetic datasets, ensuring reproducibility without external data acquisition. The combination of static validation, automated regression testing, and containerized runtime simplifies replication of the reported experiments and highlights the engineering considerations involved in operating a hybrid forecasting service.
