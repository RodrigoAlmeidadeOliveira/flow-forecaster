\section{Methodology}
We adopted an implementation-driven research strategy grounded in software repository mining and static/dynamic program analysis. The unit of analysis is the Flow Forecaster open-source project (commit snapshot November 2025), comprising a Flask web backend, Python forecasting modules, JavaScript dashboards, unit/regression tests, and deployment artifacts. We first inventoried documentation (\texttt{README.md}, \texttt{README\_ADVANCED.md}, user manuals) to elicit the claimed functionality, target users, and advertised evaluation settings. Next, we inspected core source files---notably \texttt{app.py}, \texttt{monte\_carlo\_unified.py}, \texttt{ml\_forecaster.py}, \texttt{backtesting.py}, and the JavaScript controllers under \texttt{static/js}---to recover architectural boundaries, data-processing routines, and embedded validation logic.

To characterize the experimentation workflow, we traced the API endpoints exposed in \texttt{app.py} and matched them to their analytical backends. For the Monte Carlo component, we reverse-engineered the stochastic kernel, percentiles, and risk summarization functions by reviewing \texttt{monte\_carlo\_unified.py} and the accompanying regression tests (\texttt{test\_probability\_fix.py}, \texttt{test\_deadline\_fix.py}). For the machine learning subsystem, we analyzed the class design in \texttt{ml\_forecaster.py}, including feature engineering (lagged throughput, rolling statistics), model zoo configuration, and hyperparameter search routines leveraging \texttt{TimeSeriesSplit}, \texttt{RandomizedSearchCV}, and \texttt{GridSearchCV}. We also examined the dependency-analysis hooks and portfolio analytics to understand how external risk factors propagate into forecasts.

Dynamic understanding relied on executing the documented test scripts and reading their expected outputs. The backtesting module (\texttt{backtesting.py}) provided insight into the walk-forward evaluation protocol, error metrics (MAE, RMSE, mean absolute percentage error), and backlog confidence levels (P50, P85, P95). The deadline validation script revealed how the system reconciles capacity forecasts with calendar deadlines, while probability-table tests verified percentile semantics. Although no proprietary datasets accompany the repository, the included synthetic throughput samples---typically 8--12 weekly observations per scenario---and default configuration (10\,000 Monte Carlo draws, ensemble of up to nine ML regressors) supplied a consistent basis for reconstructing the platform's experimental assumptions. Collectively, these methods enable a reproducible account of Flow Forecaster's design, evaluation, and data requirements without external instrumentation.
